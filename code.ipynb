{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf0b761",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc44327",
   "metadata": {},
   "source": [
    "## Objective\n",
    "### Automatically detect topics in a collection of news articles by analyzing the text and identifying common themes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14fbd3",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection : News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71059f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import CoherenceModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593b8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News API\n",
    "API_KEY = '019c362ec11d44fb91aa17a4b10c2d86'\n",
    "\n",
    "# Fetch news articles from News API\n",
    "def fetch_news_articles(query, num_articles=5):\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'pageSize': num_articles,  # Number of articles to retrieve\n",
    "        'apiKey': API_KEY,\n",
    "        'language': 'en',  # Language of articles\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract article content\n",
    "    articles = [article['description'] for article in data['articles'] if article['description']]\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abca3e",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51989e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\artha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\artha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Articles: [['scroll', 'found', 'shadow', 'vesuviu', 'librari', 'ancient', 'text', 'besid', 'illumin', 'machin', 'learn', 'comput', 'vision'], ['despit', 'limit', 'maker', 'mariovgg', 'think', 'ai', 'video', 'could', 'one', 'day', 'replac', 'game', 'engin'], ['upon', 'time', 'machin', 'learn', 'arcan', 'field', 'preserv', 'preciou', 'research', 'hole', 'grand', 'academ', 'institut', 'progress', 'slow', 'hard', 'today', 'howev'], ['ai', 'learn', 'play', 'simul', 'environ'], ['initi', 'iphon', 'iphon', 'pro', 'use', 'task']]\n",
      "Dictionary: {'ancient': 0, 'besid': 1, 'comput': 2, 'found': 3, 'illumin': 4, 'learn': 5, 'librari': 6, 'machin': 7, 'scroll': 8, 'shadow': 9, 'text': 10, 'vesuviu': 11, 'vision': 12, 'ai': 13, 'could': 14, 'day': 15, 'despit': 16, 'engin': 17, 'game': 18, 'limit': 19, 'maker': 20, 'mariovgg': 21, 'one': 22, 'replac': 23, 'think': 24, 'video': 25, 'academ': 26, 'arcan': 27, 'field': 28, 'grand': 29, 'hard': 30, 'hole': 31, 'howev': 32, 'institut': 33, 'preciou': 34, 'preserv': 35, 'progress': 36, 'research': 37, 'slow': 38, 'time': 39, 'today': 40, 'upon': 41, 'environ': 42, 'play': 43, 'simul': 44, 'initi': 45, 'iphon': 46, 'pro': 47, 'task': 48, 'use': 49}\n",
      "Corpus: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(5, 1), (7, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)], [(5, 1), (13, 1), (42, 1), (43, 1), (44, 1)], [(45, 1), (46, 2), (47, 1), (48, 1), (49, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stopwords, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Stopword removal\n",
    "    \n",
    "    # Apply stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "query = 'machine learning'\n",
    "articles = fetch_news_articles(query)  # Ensure articles are fetched\n",
    "processed_articles = [preprocess(article) for article in articles]\n",
    "\n",
    "# Create a dictionary that maps each word to a unique id\n",
    "dictionary = corpora.Dictionary(processed_articles)\n",
    "\n",
    "# Create a corpus: Bag of Words format for each document\n",
    "corpus = [dictionary.doc2bow(article) for article in processed_articles]\n",
    "\n",
    "print(\"Processed Articles:\", processed_articles)\n",
    "print(\"Dictionary:\", dictionary.token2id)\n",
    "print(\"Corpus:\", corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61d7fa",
   "metadata": {},
   "source": [
    "## Step 3: a) LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f73a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.065*\"iphon\" + 0.038*\"ai\" + 0.037*\"task\" + 0.037*\"pro\" + 0.037*\"use\"')\n",
      "(1, '0.038*\"machin\" + 0.038*\"learn\" + 0.038*\"today\" + 0.038*\"research\" + 0.038*\"hole\"')\n",
      "(2, '0.067*\"learn\" + 0.038*\"found\" + 0.038*\"text\" + 0.038*\"vesuviu\" + 0.038*\"librari\"')\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print topics found by the model\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c32b3",
   "metadata": {},
   "source": [
    "## Step 3: b) Word Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf6479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'iphon', 'ai', 'machin', 'vision', 'one', 'could', 'video', 'think', 'mariovgg']\n",
      "Most similar words to 'learn': [('illumin', 0.218916654586792), ('mariovgg', 0.21620631217956543), ('pro', 0.19549766182899475), ('preciou', 0.16923967003822327), ('environ', 0.1518188714981079)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec on the preprocessed articles\n",
    "word2vec_model = Word2Vec(sentences=processed_articles, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# Get all words in the vocabulary\n",
    "vocabulary = list(word2vec_model.wv.index_to_key)\n",
    "\n",
    "# Pick a word from the vocabulary to check similar words\n",
    "print(vocabulary[:10])  # Look at the first 10 words in the vocabulary\n",
    "\n",
    "# Example: Pick a word that exists in the vocabulary\n",
    "similar_words = word2vec_model.wv.most_similar(vocabulary[0], topn=5)\n",
    "print(f\"Most similar words to '{vocabulary[0]}': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b791ea",
   "metadata": {},
   "source": [
    "## Step 4: Topics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "562edb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for visualization\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the visualization in the notebook\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# Optionally, save the visualization to an HTML file\n",
    "pyLDAvis.save_html(lda_display, 'lda_visualization.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccdc75",
   "metadata": {},
   "source": [
    "## Step 5: Coherence Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7187caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5330683838522655\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_articles, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1afa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016e728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
